{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from scipy import optimize\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 420"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent method\n",
    "Since matrix $P$ is given in the problem statement, we will set it uniformly distributed. Let $n = 4$ and $m = 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07080633 0.11537703 0.22002895 0.2811566 ]\n",
      " [0.22490138 0.39550501 0.00420416 0.03564955]\n",
      " [0.23184261 0.00372862 0.26632326 0.29489023]\n",
      " [0.20151782 0.03636117 0.21661156 0.3463133 ]\n",
      " [0.27093187 0.44902816 0.29283207 0.04199032]]\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "m = 5\n",
    "P = np.array([np.random.uniform(size=n) for x in np.zeros(m)])\n",
    "P /= P.sum(axis=0)\n",
    "c_t = np.array([-np.sum(x * np.log2(x)) for x in P.T])\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    #get projection of x\n",
    "    x_ = euclidean_proj_simplex(x)\n",
    "    y = P @ x_\n",
    "    if (np.min(y) <= 0):\n",
    "        return np.inf\n",
    "    return c_t @ x_ + np.sum(y * np.log(y) / np.log(2))\n",
    "\n",
    "def grad_f(x):\n",
    "    #get projection of x\n",
    "    x_ = euclidean_proj_simplex(x)\n",
    "    y = P @ x_\n",
    "    if (np.min(y) <= 0):\n",
    "        raise ValueError\n",
    "    grad = c_t.copy()\n",
    "    tmp = []\n",
    "    for i in range(m):\n",
    "        tmp.append(P[i] * (np.log(P[i] @ x_) + 1) / np.log(2))\n",
    "    tmp_sum = np.sum(np.array(tmp), axis=0)\n",
    "    return grad + tmp_sum\n",
    "\n",
    "def gess_f(x):\n",
    "    x_ = euclidean_proj_simplex(x)\n",
    "    y = P @ x_\n",
    "    if (np.min(y) <= 0):\n",
    "        raise ValueError\n",
    "    gess = []\n",
    "    for i in range(m):\n",
    "        tmp = np.atleast_2d(P[i]).T @ np.atleast_2d(P[i])\n",
    "        gess.append(tmp / (y[i] * np.log(2)))\n",
    "    return np.sum(np.array(gess), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_proj_simplex(v, s=1):\n",
    "    n, = v.shape  \n",
    "    if v.sum() == s and np.alltrue(v >= 0):\n",
    "        return v\n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u)\n",
    "    rho = np.nonzero(u * np.arange(1, n+1) > (cssv - s))[0][-1]\n",
    "    theta = (cssv[rho] - s) / (rho + 1.0)\n",
    "    w = (v - theta).clip(min=0)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoppingCriteria:\n",
    "    def __init__(self, max_iterations=np.inf, min_grad_norm=0):\n",
    "        self.max_iterations = max_iterations\n",
    "        self.min_grad_norm = min_grad_norm\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        cur_iterations = state['iterations']\n",
    "        cur_grad_norm = np.linalg.norm(state['cur_grad'], ord=2)\n",
    "        dif_x = np.linalg.norm(state['x'] - state['prev_x'], ord=2)\n",
    "        return (cur_iterations >= self.max_iterations or \n",
    "                cur_grad_norm <= self.min_grad_norm or dif_x <= self.min_grad_norm)\n",
    "    \n",
    "class StoppingCriteriaNewton:\n",
    "    def __init__(self, max_iterations=np.inf, tolerance=10**(-4), min_grad_norm=0):\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.min_grad_norm = min_grad_norm\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        cur_iterations = state['iterations']\n",
    "        dif_x = np.linalg.norm(state['x'] - state['prev_x'], ord=2)\n",
    "        cur_decrement = state['cur_grad'].T @ state['cur_gess_inv'] @ state['cur_grad']\n",
    "        return (cur_iterations >= self.max_iterations or \n",
    "                cur_decrement/2 <= self.tolerance or dif_x <= self.min_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepSearchFastestTernary:\n",
    "    def __init__(self, precision):\n",
    "        self.precision = precision\n",
    "        self.left = 0\n",
    "        self.right = None\n",
    "        \n",
    "    def __update_starting_points(self, state, init_kpower=-2):\n",
    "        k_power = init_kpower\n",
    "        f = state['f']\n",
    "        x = state['x']\n",
    "        grad_f = state['cur_grad']\n",
    "        dx = - grad_f\n",
    "        while f(x + 2**k_power * dx) > f(x + 2**(k_power + 1) * dx):\n",
    "            k_power += 1\n",
    "        if k_power == init_kpower:\n",
    "            self.left = 0\n",
    "        else:\n",
    "            self.left = 2**(k_power - 1)\n",
    "        self.right = 2**(k_power + 1)\n",
    "            \n",
    "            \n",
    "    def __call__(self, state):\n",
    "        f = state['f']\n",
    "        x = state['x']\n",
    "        cur_grad = state['cur_grad']\n",
    "        dx = -cur_grad\n",
    "        \n",
    "        self.__update_starting_points(state) # update self.left and self.right\n",
    "        \n",
    "        right = self.right\n",
    "        left = self.left\n",
    "        \n",
    "        while True:\n",
    "            if abs(right - left) < self.precision:\n",
    "                return (left + right)/2\n",
    "\n",
    "            left_div = left + (right - left)/3\n",
    "            right_div = right - (right - left)/3\n",
    "\n",
    "            f_left = f(x + left_div * dx)\n",
    "            f_right = f(x + right_div * dx)\n",
    "            \n",
    "            if f_left == np.inf:\n",
    "                right = right_div\n",
    "            else:\n",
    "                if f_left < f_right:\n",
    "                    right = right_div\n",
    "                else:\n",
    "                    left = left_div\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentMethod:\n",
    "    def __init__(self, t_search, stopping_criteria):\n",
    "        self.t_search = t_search\n",
    "        self.stopping_criteria = stopping_criteria\n",
    "    \n",
    "    def fit(self, f, grad_f, x_0):\n",
    "        x = x_0.copy()\n",
    "        state = dict()\n",
    "        state['f'] = f\n",
    "        state['grad_f'] = grad_f\n",
    "        state['x'] = x\n",
    "        # hardcoded xD\n",
    "        state['prev_x'] = np.ones(n) / n\n",
    "        \n",
    "        state['iterations'] = 0\n",
    "        state['time'] = time.time()\n",
    "        while True:\n",
    "            state['cur_grad'] = grad_f(state['x'])\n",
    "            if self.stopping_criteria(state):\n",
    "                break\n",
    "            t = self.t_search(state)\n",
    "            state['prev_x'] = state['x'].copy()\n",
    "            state['x'] -= t * state['cur_grad']\n",
    "            #take projection on simplex\n",
    "            state['x'] = euclidean_proj_simplex(state['x'])\n",
    "            state['iterations'] += 1\n",
    "            \n",
    "        state['time'] = time.time() - state['time']\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewtonMethod:\n",
    "    def __init__(self, t_search, stopping_criteria):\n",
    "        self.t_search = t_search\n",
    "        self.stopping_criteria = stopping_criteria\n",
    "    \n",
    "    def fit(self, f, grad_f, gess_f, x_0):\n",
    "        x = x_0.copy()\n",
    "        state = dict()\n",
    "        state['f'] = f\n",
    "        state['grad_f'] = grad_f\n",
    "        state['x'] = x\n",
    "        # hardcoded xD\n",
    "        state['prev_x'] = np.ones(n) / n\n",
    "        \n",
    "        state['iterations'] = 0\n",
    "        state['time'] = time.time()\n",
    "        while True:\n",
    "            state['cur_grad'] = grad_f(state['x'])\n",
    "            state['cur_gess'] = gess_f(state['x'])\n",
    "            state['cur_gess_inv'] = np.linalg.inv(state['cur_gess'])\n",
    "\n",
    "            if self.stopping_criteria(state):\n",
    "                break\n",
    "                \n",
    "            step = -state['cur_gess_inv'] @ grad_f(state['x'])\n",
    "            t = self.t_search(state)\n",
    "            state['prev_x'] = state['x'].copy()\n",
    "            state['x'] += t * step\n",
    "            #take projection on simplex\n",
    "            state['x'] = euclidean_proj_simplex(state['x'])\n",
    "            state['iterations'] += 1\n",
    "            \n",
    "        state['time'] = time.time() - state['time']\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_0 =  [0.2824378  0.19136063 0.40211072 0.12409085]\n",
      "f_min = -0.534234052467002\n",
      "time =  0.02068614959716797\n",
      "x =  [0.         0.50339697 0.         0.49660303]\n"
     ]
    }
   ],
   "source": [
    "stopping_criteria = StoppingCriteria(min_grad_norm=1e-7)\n",
    "t_search = StepSearchFastestTernary(precision=1e-7)\n",
    "grad = GradientDescentMethod(t_search=t_search, stopping_criteria=stopping_criteria)\n",
    "x_0 = np.random.uniform(low=0, high=1, size=n)\n",
    "x_0 /= np.sum(x_0)\n",
    "state = grad.fit(f, grad_f, x_0)\n",
    "print('x_0 = ', x_0)\n",
    "print('f_min =', f(state['x']))\n",
    "print('time = ', state['time'])\n",
    "print('x = ',state['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_0 =  [0.2824378  0.19136063 0.40211072 0.12409085]\n",
      "f_min = -0.532982717694676\n",
      "time =  0.0819556713104248\n",
      "x =  [0.         0.47697571 0.         0.52302429]\n"
     ]
    }
   ],
   "source": [
    "stopping_criteria = StoppingCriteriaNewton(max_iterations=10**3)\n",
    "t_search = StepSearchFastestTernary(precision=1e-7)\n",
    "newton = NewtonMethod(t_search=t_search, stopping_criteria=stopping_criteria)\n",
    "state = newton.fit(f, grad_f, gess_f, x_0)\n",
    "print('x_0 = ', x_0)\n",
    "print('f_min =', f(state['x']))\n",
    "print('time = ', state['time'])\n",
    "print('x = ',state['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = ({'type': 'eq', 'fun': lambda x:  np.sum(x) - 1},\n",
    "        {'type': 'ineq', 'fun': lambda x: x[0]},\n",
    "        {'type': 'ineq', 'fun': lambda x: x[1]},\n",
    "       {'type': 'ineq', 'fun': lambda x: x[2]},\n",
    "       {'type': 'ineq', 'fun': lambda x: x[3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: -0.5342340524669997\n",
      "     jac: array([ 2.83214331e-01,  5.96046448e-08,  1.24464795e-01, -1.49011612e-08])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 36\n",
      "     nit: 6\n",
      "    njev: 6\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-6.72205380e-18,  5.03397004e-01,  7.25598544e-18,  4.96602996e-01])\n"
     ]
    }
   ],
   "source": [
    "print(optimize.minimize(f, x_0, method='SLSQP',\n",
    "               constraints=cons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
