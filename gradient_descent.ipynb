{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from scipy import optimize\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 420"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent method\n",
    "Since matrix $P$ is given in the problem statement, we will set it uniformly distributed. Let $n = 4$ and $m = 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.36366239  0.1375436   0.11298746  0.15422615]\n",
      " [ 0.06170611  0.20613491  0.270838    0.18208947]\n",
      " [ 0.40068035  0.0850128   0.22377749  0.09740263]\n",
      " [ 0.11047012  0.31705305  0.30961554  0.06979643]\n",
      " [ 0.06348103  0.25425564  0.0827815   0.49648532]]\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "m = 5\n",
    "P = np.array([np.random.uniform(size=n) for x in np.zeros(m)])\n",
    "P /= P.sum(axis=0)\n",
    "c_t = np.array([-np.sum(x * np.log2(x)) for x in P.T])\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    #get projection of x\n",
    "    x_ = euclidean_proj_simplex(x)\n",
    "    y = P @ x_\n",
    "    if (np.min(y) <= 0):\n",
    "        return np.inf\n",
    "    return c_t @ x_ + np.sum(y * np.log(y) / np.log(2))\n",
    "\n",
    "def grad_f(x):\n",
    "    #get projection of x\n",
    "    x_ = euclidean_proj_simplex(x)\n",
    "    y = P @ x_\n",
    "    if (np.min(y) <= 0):\n",
    "        raise ValueError\n",
    "    grad = c_t.copy()\n",
    "    tmp = []\n",
    "    for i in range(m):\n",
    "        tmp.append(P[i] * (np.log(P[i] @ x_) + 1) / np.log(2))\n",
    "    tmp_sum = np.sum(np.array(tmp), axis=0)\n",
    "    return grad + tmp_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_proj_simplex(v, s=1):\n",
    "    n, = v.shape  \n",
    "    if v.sum() == s and np.alltrue(v >= 0):\n",
    "        return v\n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u)\n",
    "    rho = np.nonzero(u * np.arange(1, n+1) > (cssv - s))[0][-1]\n",
    "    theta = (cssv[rho] - s) / (rho + 1.0)\n",
    "    w = (v - theta).clip(min=0)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoppingCriteria:\n",
    "    def __init__(self, max_iterations=np.inf, min_grad_norm=0):\n",
    "        self.max_iterations = max_iterations\n",
    "        self.min_grad_norm = min_grad_norm\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        cur_iterations = state['iterations']\n",
    "        cur_grad_norm = np.linalg.norm(state['cur_grad'], ord=2)\n",
    "        dif_z = np.linalg.norm(state['z'] - state['prev_z'], ord=2)\n",
    "        return (cur_iterations >= self.max_iterations or cur_grad_norm <= self.min_grad_norm or dif_z <= self.min_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepSearchFastestTernary:\n",
    "    def __init__(self, precision):\n",
    "        self.precision = precision\n",
    "        self.left = 0\n",
    "        self.right = None\n",
    "        \n",
    "    def __update_starting_points(self, state, init_kpower=-2):\n",
    "        k_power = init_kpower\n",
    "        f = state['f']\n",
    "        z = state['z']\n",
    "        grad_f = state['cur_grad']\n",
    "        dx = - grad_f\n",
    "        while f(z + 2**k_power * dx) > f(z + 2**(k_power + 1) * dx):\n",
    "            k_power += 1\n",
    "        if k_power == init_kpower:\n",
    "            self.left = 0\n",
    "        else:\n",
    "            self.left = 2**(k_power - 1)\n",
    "        self.right = 2**(k_power + 1)\n",
    "            \n",
    "            \n",
    "    def __call__(self, state):\n",
    "        f = state['f']\n",
    "        z = state['z']\n",
    "        cur_grad = state['cur_grad']\n",
    "        dx = -cur_grad\n",
    "        \n",
    "        self.__update_starting_points(state) # update self.left and self.right\n",
    "        \n",
    "        right = self.right\n",
    "        left = self.left\n",
    "        \n",
    "        while True:\n",
    "            if abs(right - left) < self.precision:\n",
    "                return (left + right)/2\n",
    "\n",
    "            left_div = left + (right - left)/3\n",
    "            right_div = right - (right - left)/3\n",
    "\n",
    "            f_left = f(z + left_div * dx)\n",
    "            f_right = f(z + right_div * dx)\n",
    "            \n",
    "            if f_left == np.inf:\n",
    "                right = right_div\n",
    "            else:\n",
    "                if f_left < f_right:\n",
    "                    right = right_div\n",
    "                else:\n",
    "                    left = left_div\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentMethod:\n",
    "    def __init__(self, t_search, stopping_criteria):\n",
    "        self.t_search = t_search\n",
    "        self.stopping_criteria = stopping_criteria\n",
    "    \n",
    "    def fit(self, f, grad_f, z_0):\n",
    "        z = z_0.copy()\n",
    "        state = dict()\n",
    "        state['f'] = f\n",
    "        state['grad_f'] = grad_f\n",
    "        state['z'] = z\n",
    "        # hardcoded \n",
    "        state['prev_z'] = np.ones(n) / n\n",
    "        state['iterations'] = 0\n",
    "        state['time'] = time.time()\n",
    "        while True:\n",
    "            state['cur_grad'] = grad_f(state['z'])\n",
    "            if self.stopping_criteria(state):\n",
    "                break\n",
    "            t = self.t_search(state)\n",
    "            state['prev_z'] = state['z'].copy()\n",
    "            state['z'] -= t * state['cur_grad']\n",
    "            #take projection on simplex\n",
    "            state['z'] = euclidean_proj_simplex(state['z'])\n",
    "            state['iterations'] += 1\n",
    "            \n",
    "        state['time'] = time.time() - state['time']\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_0 =  [ 0.36451531  0.03186695  0.27508598  0.32853177]\n",
      "f_min = -0.298013392595\n",
      "time =  0.059381961822509766\n",
      "z =  [ 0.38330255  0.          0.23675372  0.37994374]\n"
     ]
    }
   ],
   "source": [
    "stopping_criteria = StoppingCriteria(min_grad_norm=1e-7)\n",
    "t_search = StepSearchFastestTernary(precision=1e-7)\n",
    "grad = GradientDescentMethod(t_search=t_search, stopping_criteria=stopping_criteria)\n",
    "z_0 = np.random.uniform(low=0, high=1, size=n)\n",
    "z_0 /= np.sum(z_0)\n",
    "state = grad.fit(f, grad_f, z_0)\n",
    "print('z_0 = ', z_0)\n",
    "print('f_min =', f(state['z']))\n",
    "print('time = ', state['time'])\n",
    "print('z = ',state['z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = ({'type': 'eq', 'fun': lambda x:  np.sum(x) - 1},\n",
    "        {'type': 'ineq', 'fun': lambda x: x[0]},\n",
    "        {'type': 'ineq', 'fun': lambda x: x[1]},\n",
    "       {'type': 'ineq', 'fun': lambda x: x[2]},\n",
    "       {'type': 'ineq', 'fun': lambda x: x[3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: -0.298012922964378\n",
      "     jac: array([ -6.03929162e-04,   3.33339423e-02,   5.40405512e-04,\n",
      "         6.35832548e-05])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 18\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([  3.82490710e-01,   4.63496429e-18,   2.37586949e-01,\n",
      "         3.79922341e-01])\n"
     ]
    }
   ],
   "source": [
    "print(optimize.minimize(f, z_0, method='SLSQP',\n",
    "               constraints=cons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
