{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from scipy import optimize\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed = 420"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent method\n",
    "Since matrix $P$ is given in the problem statement, we will set it uniformly distributed. Let $n = 4$ and $m = 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.21835855  0.33189874  0.36619697  0.26443389]\n",
      " [ 0.17575225  0.03124171  0.13026456  0.11555428]\n",
      " [ 0.2897017   0.05940226  0.25613704  0.14353018]\n",
      " [ 0.16308107  0.22573601  0.12051723  0.27849854]\n",
      " [ 0.15310643  0.35172127  0.1268842   0.19798311]]\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "m = 5\n",
    "P = np.array([np.random.uniform(size=n) for x in np.zeros(m)])\n",
    "P /= P.sum(axis=0)\n",
    "c_t = np.array([-np.sum(x * np.log2(x)) for x in P.T])\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    #get projection of x\n",
    "    x_ = euclidean_proj_simplex(x)\n",
    "    y = P @ x_\n",
    "    if (np.min(y) <= 0):\n",
    "        return np.inf\n",
    "    return c_t @ x_ + np.sum(y * np.log(y) / np.log(2))\n",
    "\n",
    "def grad_f(x):\n",
    "    #get projection of x\n",
    "    x_ = euclidean_proj_simplex(x)\n",
    "    y = P @ x_\n",
    "    if (np.min(y) <= 0):\n",
    "        raise ValueError\n",
    "    grad = c_t.copy()\n",
    "    tmp = []\n",
    "    for i in range(m):\n",
    "        tmp.append(P[i] * (np.log(P[i] @ x_) + 1) / np.log(2))\n",
    "    tmp_sum = np.sum(np.array(tmp), axis=0)\n",
    "    return grad + tmp_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_proj_simplex(v, s=1):\n",
    "    n, = v.shape  \n",
    "    if v.sum() == s and np.alltrue(v >= 0):\n",
    "        return v\n",
    "    u = np.sort(v)[::-1]\n",
    "    cssv = np.cumsum(u)\n",
    "    rho = np.nonzero(u * np.arange(1, n+1) > (cssv - s))[0][-1]\n",
    "    theta = (cssv[rho] - s) / (rho + 1.0)\n",
    "    w = (v - theta).clip(min=0)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoppingCriteria:\n",
    "    def __init__(self, max_iterations=np.inf, min_grad_norm=0):\n",
    "        self.max_iterations = max_iterations\n",
    "        self.min_grad_norm = min_grad_norm\n",
    "    \n",
    "    def __call__(self, state):\n",
    "        cur_iterations = state['iterations']\n",
    "        cur_grad_norm = np.linalg.norm(state['cur_grad'], ord=2)\n",
    "        dif_x = np.linalg.norm(state['x'] - state['prev_x'], ord=2)\n",
    "        return (cur_iterations >= self.max_iterations or cur_grad_norm <= self.min_grad_norm or dif_x <= self.min_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepSearchFastestTernary:\n",
    "    def __init__(self, precision):\n",
    "        self.precision = precision\n",
    "        self.left = 0\n",
    "        self.right = None\n",
    "        \n",
    "    def __update_starting_points(self, state, init_kpower=-2):\n",
    "        k_power = init_kpower\n",
    "        f = state['f']\n",
    "        x = state['x']\n",
    "        grad_f = state['cur_grad']\n",
    "        dx = - grad_f\n",
    "        while f(x + 2**k_power * dx) > f(x + 2**(k_power + 1) * dx):\n",
    "            k_power += 1\n",
    "        if k_power == init_kpower:\n",
    "            self.left = 0\n",
    "        else:\n",
    "            self.left = 2**(k_power - 1)\n",
    "        self.right = 2**(k_power + 1)\n",
    "            \n",
    "            \n",
    "    def __call__(self, state):\n",
    "        f = state['f']\n",
    "        x = state['x']\n",
    "        cur_grad = state['cur_grad']\n",
    "        dx = -cur_grad\n",
    "        \n",
    "        self.__update_starting_points(state) # update self.left and self.right\n",
    "        \n",
    "        right = self.right\n",
    "        left = self.left\n",
    "        \n",
    "        while True:\n",
    "            if abs(right - left) < self.precision:\n",
    "                return (left + right)/2\n",
    "\n",
    "            left_div = left + (right - left)/3\n",
    "            right_div = right - (right - left)/3\n",
    "\n",
    "            f_left = f(x + left_div * dx)\n",
    "            f_right = f(x + right_div * dx)\n",
    "            \n",
    "            if f_left == np.inf:\n",
    "                right = right_div\n",
    "            else:\n",
    "                if f_left < f_right:\n",
    "                    right = right_div\n",
    "                else:\n",
    "                    left = left_div\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentMethod:\n",
    "    def __init__(self, t_search, stopping_criteria):\n",
    "        self.t_search = t_search\n",
    "        self.stopping_criteria = stopping_criteria\n",
    "    \n",
    "    def fit(self, f, grad_f, x_0):\n",
    "        x = x_0.copy()\n",
    "        state = dict()\n",
    "        state['f'] = f\n",
    "        state['grad_f'] = grad_f\n",
    "        state['x'] = x\n",
    "        # hardcoded xD\n",
    "        state['prev_x'] = np.ones(n) / n\n",
    "        \n",
    "        state['iterations'] = 0\n",
    "        state['time'] = time.time()\n",
    "        while True:\n",
    "            state['cur_grad'] = grad_f(state['x'])\n",
    "            if self.stopping_criteria(state):\n",
    "                break\n",
    "            t = self.t_search(state)\n",
    "            state['prev_x'] = state['x'].copy()\n",
    "            state['x'] -= t * state['cur_grad']\n",
    "            #take projection on simplex\n",
    "            state['x'] = euclidean_proj_simplex(state['x'])\n",
    "            state['iterations'] += 1\n",
    "            \n",
    "        state['time'] = time.time() - state['time']\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_0 =  [ 0.13952584  0.39651145  0.33776761  0.12619511]\n",
      "f_min = -0.141671195045\n",
      "time =  0.22211384773254395\n",
      "x =  [ 0.46340459  0.53659541  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "stopping_criteria = StoppingCriteria(min_grad_norm=1e-7)\n",
    "t_search = StepSearchFastestTernary(precision=1e-7)\n",
    "grad = GradientDescentMethod(t_search=t_search, stopping_criteria=stopping_criteria)\n",
    "x_0 = np.random.uniform(low=0, high=1, size=n)\n",
    "x_0 /= np.sum(x_0)\n",
    "state = grad.fit(f, grad_f, x_0)\n",
    "print('x_0 = ', x_0)\n",
    "print('f_min =', f(state['x']))\n",
    "print('time = ', state['time'])\n",
    "print('x = ',state['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cons = ({'type': 'eq', 'fun': lambda x:  np.sum(x) - 1},\n",
    "        {'type': 'ineq', 'fun': lambda x: x[0]},\n",
    "        {'type': 'ineq', 'fun': lambda x: x[1]},\n",
    "       {'type': 'ineq', 'fun': lambda x: x[2]},\n",
    "       {'type': 'ineq', 'fun': lambda x: x[3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: -0.1416711939597448\n",
      "     jac: array([ -2.42292881e-05,   2.42590904e-05,   1.13973022e-03,\n",
      "         6.89630210e-02])\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 48\n",
      "     nit: 8\n",
      "    njev: 8\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([  4.63359779e-01,   5.36640221e-01,  -1.86482774e-17,\n",
      "         8.67361738e-19])\n"
     ]
    }
   ],
   "source": [
    "print(optimize.minimize(f, x_0, method='SLSQP',\n",
    "               constraints=cons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
